---
title: "p8105_hw2_as6447"
author: "Armaan Sodhi"
date: "2022-10-03"
output: github_document
---

##Problem 0

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(readxl)
library(lubridate)
```

##Problem 1

```{r cars}
NYC_transit_data = 
  read_csv('data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv',
           col_types = cols(Route8 = "c", Route9 = "c", Route10 = "c", Route11 = "c")) %>%
  janitor::clean_names()%>%
  select( 
          line, station_name, station_latitude, station_longitude, 
          entry, vending, exit_only, entrance_type, ada, starts_with('route'))%>%
  mutate(entry=if_else(entry=='YES',TRUE,FALSE))

nrow(NYC_transit_data)
ncol(NYC_transit_data)
```

From the use of `nrow` and `ncol` we find that there are 20 columns (variables) and 1868 rows (observations). It provides geographic data about where stations are located (ie station name, longitude/latitude), the characteristics of those locations (ie what type of entry do they have, are they ADA compliant, are there vending machines in the station), and the subway lines that run through the stations and the routes they follow. Cleaning the data required converting the variables to snake case (so that I can actually manipulate the variables without increasing my blood pressure), then I restricted the data set to only the required variables. Finally, I converted the entry variable to a logical variable.

The data itself is not tidy considering the routes variables which are in `pivot_wider` so that each route is its own variable. It would be more efficient and "tidier" to `Pivot_longer`.

```{r}
NYC_transit_data %>% distinct(line,station_name)
```

There are 465 unique stations (found in the console after running the above code chunk)

```{r}
NYC_transit_data %>% filter(ada == TRUE) %>% distinct(line,station_name)
```

84 stations are found to be ADA Compliant.

```{r}
NYC_transit_data %>% 
  filter(vending =='NO')%>%
  pull(entry)%>%
  mean()
```

The proportion of station entrances/exit without vending allow entrances is 0.377049 (37.70% of station entrances and exits are without vending allowed)

```{r}
NYC_transit_data %>% 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") %>% 
  filter(route == "A") %>% 
  select(station_name, line) %>% 
  distinct

NYC_transit_data %>% 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") %>% 
  filter(route == "A", ada == TRUE) %>% 
  select(station_name, line) %>% 
  distinct
```

There are 60 stations and lines that serve the A train and 17 stations that are ADA compliant.

##Problem 2

```{r Mr trash wheel import,clean,organize}
Mr_Trash_Wheel = 
  readxl::read_excel("data/Trash Wheel Collection Data.xlsx", "Mr. Trash Wheel", range = "A2:N549")%>%
  janitor::clean_names()%>%
  drop_na()%>%
  mutate(
    sports_balls=round(sports_balls),
    sports_balls= as.integer(sports_balls),
    sheet_type= 'Mr_trash_wheel'
    )
```

This sheet initially has 548 observations and we have been given 16 variables, two of which where empty variables. We removed these variables and then ended up with only 14 variables. We omitted any rows that did not include dumpster-specific data. I then rounded and then converted it to an integer.

```{r professor trash wheel import,clean,organize}
Professor_Trash_Wheel = 
  readxl::read_excel("data/Trash Wheel Collection Data.xlsx", "Professor Trash Wheel", range = "A2:M96")%>%
  janitor::clean_names()%>%
  drop_na()%>%
  mutate(
    year = as.character(year),
    sheet_type = 'Professor_trash_wheel'
  )
```

Please note that I converted the year variable to a character here so that the two data sets are consistent with each other. I then created the `sheet_type` variable so that I could properly differentiate between the two when I merged them later. I dropped any rows that did not include dumpster-specific data as well. And converted the variables to snake case.

```{r bind rows}
trash_wheel_bind =
  bind_rows(Professor_Trash_Wheel, Mr_Trash_Wheel)
```

```{r total trash by professor}
trash_wheel_bind %>% 
  filter(sheet_type == 'Professor_trash_wheel')%>%
  pull(weight_tons)%>%
  sum()
```

```{r total sports balls by mr}
trash_wheel_bind %>% 
  filter(sheet_type == 'Mr_trash_wheel', year=='2020')%>%
  pull(sports_balls)%>%
  sum()
```

This consisted of data about a water wheel of two different types. There was two different wheels,`Mr_Trash_Wheel` (468 rows and 15 columns) and `Professor_Trash_Wheel` (82 rows and 14 variables). Key variables would include the `dumpster`(dumpster number), `weight_tons` (how much weight of trash in tons ), `sports_balls`(how many sports balls were collected), and `volume_cubic_yards` (the volume of trash in cubic yards that is thrown away). When the two are binded together it creates a `trash_wheel_bind` (568 rows and 15 columns). The total weight of trash collected by Professor Trash Wheel was 162.54 tons. In 2020, `Mr. Trash Wheel` collected 856 sports balls.

##Problem 3

```{r pols}
pols_month_data = 
  read_csv('data/pols-month.csv')%>%
  janitor::clean_names()%>%
  separate(col = mon, into=c('year','month','day'), sep='-')%>%
  mutate(
    month = as.integer(month),
    month = month.abb[month],
    year = as.integer(year),
    president = case_when( 
      prez_dem == 1 ~ 'dem',
      prez_gop == 1 ~ 'gop',
      prez_gop == 2 ~ 'unknown'))%>%
  select(-day,-prez_dem,-prez_gop)
```

I separated the date data by year,month, and day. Then used mutate to change the month from numbered month to character month as well as changing year to numeric (so I could merge the datasets). Then created the president variable by using `case_when` to combine `prez_dem` and `prez_gop`. Finally, I used `select` to remove `day,prez_dem,prez_gop`.

```{r snp}
snp_stock_market_index = read_csv('data/snp.csv')%>%
  janitor::clean_names()%>%
  separate(col = date, into = c('month','day','year'), sep='/')%>%
  mutate(
    month = as.integer(month),
    month = month.abb[month],
    year = as.integer(year),
    closing_value = close,
    year_fix= case_when( year<22 ~ 2000,
                         year>22 ~ 1900),
    year= year+year_fix )%>%
  relocate (year,month)%>%
select(-day,-year_fix,-close)
```

I separated the date data by year,month, and day. Then used mutate to change the month from numbered month to character month as well as changing year to numeric (so I could merge the datasets).Renamed `close` to `closing_value`. Then I fixed `year` variable as it was 2 digits instead of 4 digits by adding either 2000 or 1900 if the year was greater than or equal to 22. Then the year and month were relocated to the front and finally `day`, `year_fix`, and the original `close` value was deleted.

```{r}
unemployment_data=read_csv('data/unemployment.csv')%>%
  janitor::clean_names()%>%
pivot_longer(
  jan:dec,
  names_to = 'month', 
    values_to = 'unemployment_percent'
)%>%
  mutate(
    month=str_to_title(month)
  )

```

For the `unemployment_data` by using the `pivot_longer` function then selected january to december creating two variables `month` and `unemployment_percent`. Finally `month` was made to have the first letter capitalized.

```{r join data together}
overall_merged_Data =
  left_join(pols_month_data,snp_stock_market_index)%>%
  left_join(.,unemployment_data)
```

These datasheets consist of election data. It consists of information on politicians at the national level at a month level (ie `pols-months`-822 rows and 9 columns), the S&P stock index (`snp_stock_market_index`- 787 rows and 3 columns), and the unemployment percentages per month (`unemployment_data`-816 rows and 3 columns). The combined data consists of 822 observations with 11 different variables. These variables include `year` (1947-2022), `month`, `closing_value` (closing value of S&P index), and `president` (which political party was the president affiliated with). There were also variables for the number of: governor's, representatives, and senators that were affiliated with a particular political party.
